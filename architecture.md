
## Did you design your solution with classes?

```
Answer here.
No. I didn't design my solution with classes.

Explain why or why not.
The the workload of assignment 3 is not as much as a project, so that it is convenient for us to only use function to read and preprocess the data, count n-grams, build the models and compute the perplexity.
In my code, there are only two functions evaluate_model and train_model function. If we use the class, we may have Train class and Eval class and each class only contain one function which may cause the redundancy.
Overall, it's better to use functions rather than classes. 
```

## Does the use of classes mean you have to type more code?

```
Answer here.
In this assignment, we need to type more code.

Explain why or why not.
We may type more code for assignment 3 since the workload of the assignment is small and we don't need to maintain the structure of the code by using the classes.
Classes are very useful for a large project since it can bundle several related objects into a class to reduce the difficulty of maintenance of the project.
However, if we use classes, we may use an __init__ method to store attributes and some additional functions for preprocessing the data, counting n-grams, building the models and computing the perplexity.
Overall, using classes will increase the amount of the code.
```

---

## Functions used for model training

#### Functions you wrote with their equivalent in the LLM code (or N/A if applicable)

```
Answer here. Copy this section as needed
train_model(input_file, output_file)
- LLM equivalent: read_utterances(path), build_vocab(utterances), replace_singletons_with_unk(utterances, vocab), count_ngrams(utterances), convert_to_json(counter), create_model_json(uni, bi , tri, vocab, n_utterances)
```

#### Functions the LLM wrote that have no equivalent in your code

```
Answer here. Copy the section as needed. Write N/A if there are none.
N/A
```

#### Discussion of the similarities and differences between the two designs. 

```
Answer here.
Why are the designs similar (or different)?
Similarities:
(1) read and preprocess the utterances in training stage (eg. adding <s> and </s> for each utterance, replacing the low frequency tokens by <unk>) 
(2) count unigrams, bigrams, and trigrams (eg. using the dictionary type to count)
(3) save the model with JSON format

Differences:
(1) My code uses one main functions for training, while chatgpt code uses several sub-functions for training. 
(2) The json format of n-gram model generated by chatgpt has "meta" information, while the n-gram model generated by my code doesn't have "meta" information.
(3) In my code, the unk threshold is 20 while it is 1 in chatgpt code.
(4) In my code, the tokens in bigram and trigram are splitted by '\t' , while in chatgpt code the tokens are splitted by ' '.
(5) In my code, it turn the utterance to be utterance, ['<s>'] + utterance + ['</s>'], ['<s>', '<s>'] + utterance + ['</s>', '</s>'] for unigram, bigram, trigram model respectively in training stage.
    In chatgpt code, it turn the utterance to be ['<s>'] + utterance + ['</s>'] for unigram, bigram, trigram model in training stage.
```

#### Which design is better?

```
Answer here.
My code

Base your answer on your experience with programming.
Consider readability, efficiency, ease of debugging and other criteria you find important.

Readability: My code is better since the codes prints the additional printing additional information (ex: “done reading file”).
Efficiency: My code has a higher effiency since we have a higher unk threshold so that the vocabulary size is lower than the one in chatgpt code.
            Moreover, it has a lower data sparsity in my code than in chatgpt code.
Ease of debugging: My code is easy to debug since it provides the staus of the running code so that we can know where the bug is in the code.
Correctness: My code uses different padding strategies for unigram, bigram, trigram, while chatgpt code uses only one strategy for unigram, bigram, trigram. 
```

---

## Functions used for calculating perplexity

#### Functions you wrote with their equivalent in the LLM code (or N/A if applicable)

```
Answer here. Copy this section as needed
evaluate_model(method, vocab_file, test_file, smooth)
- LLM equivalent: read_utterances(path), map_oov(tokens, vocab), load_model(path), p_unigram(w, model), p_bigram(w1 , w2, model, laplace), p_trigram(w1, w2, w3, model, laplace)
```

#### Functions the LLM wrote that have no equivalent in your code

```
Answer here. Copy the section as needed. Write N/A if there are none.
N/A
```

#### Discussion of the similarities and differences between the two designs. 

```
Answer here.
Why are the designs similar (or different)?
Similarities:
(1) read and preprocess the utterances in eval stage (eg. replacing oov tokens by <unk>)
(2) calculate the non-zero probability of each token depending on the given n-gram method (unigram, bigram, trigram) and smooth method (laplace)

Differences:
(1) My code uses one main functions for evaluating, while chatgpt code uses several sub-functions for evaluating.
(2) In my code, when computing perplexity, it will use Back-off algorithms when the probability of the n-gram is 0 to set it to 1/|V| where |V| is the vocabulary size.
    However, in chatgpt code, when computing perplexity, it will set the perplexity score to be 'inf' when the probability of the n-gram is 0.
(3) In my code, it turns the utterance to be utterance, ['<s>'] + utterance + ['</s>'], ['<s>', '<s>'] + utterance + ['</s>', '</s>'] for unigram, bigram, trigram model respectively in eval stage.
    In chatgpt code, it turn the utterance to be ['<s>'] + utterance + ['</s>'] for unigram, bigram, trigram model in eval stage.
```

#### Which design is better?

```
Answer here. 
Base your answer on your experience with programming.
Consider readability, efficiency, ease of debugging and other criteria you find important.

Readability: My code is better since the codes prints the additional printing additional information (ex: “done reading file”).
Efficiency: My code has a higher effiency since we have a higher unk threshold so that the vocabulary size is lower than the one in chatgpt code.
Ease of debugging: My code is easy to debug since it provides the staus of the running code so that we can know where the bug is in the code.
Correctness: My code uses different padding strategies for unigram, bigram, trigram, while chatgpt code uses only one strategy for unigram, bigram, trigram. 
```


---

## Functions used for printing results

#### Functions you wrote with their equivalent in the LLM code (or N/A if applicable)

```
Answer here. Copy this section as needed
train_model(input_file, output_file)
- LLM equivalent: main()

evaluate_model(method, vocab_file, test_file, smooth)
- LLM equivalent: main()

```

#### Functions the LLM wrote that have no equivalent in your code

```
Answer here. Copy the section as needed. Write N/A if there are none.
In train.py
read_utterances(path), build_vocab(utterances), replace_singletons_with_unk(utterances, vocab), count_ngrams(utterances), convert_to_json(counter), create_model_json(uni, bi, tri, vocab, n_utterances)
In eval.py
read_utterances(path), map_oov(tokens, vocab), load_model(path), p_unigram(w, model), p_bigram(w1, w2, model, laplace), p_trigram(w1, w2, w3, model, laplace), compute_ppl(model_type, model, utterances, laplace, vocab)
```

#### Discussion of the similarities and differences between the two designs. 

```
Answer here.
Why are the designs similar (or different)?
Similarities:
(1) print the "model saved" status in training stage.
(2) print the perplexity of the given model depending on the given arguments.

Differences:
(1) My code prints "Processing utterances" and Model Statistics" status in training stage while chatgpt code doesn't.
(2) My code prints " Initalization of Data", "Processing Files & Transforming to Phonemes", "Splitting Data", " Preparing Files", status in evaluating stage while chatgpt doesn't.
```

#### Which design is better?

```
Answer here. 
Base your answer on your experience with programming.
Consider readability, efficiency, ease of debugging and other criteria you find important.
Readability: My code is better since the codes prints the additional printing additional information (ex: “done reading file”)
Ease of debugging: My code is easy to debug since it provides the staus of the running code so that we can know where the bug is in the code.
```
